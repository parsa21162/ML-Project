{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification - Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Course:** Artificial Intelligence  \n",
    "**Instructor:** Dr. Pishgoo  \n",
    "**Project Supervisor:** Eng. Alireza Ghorbani\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Load Data](#load-data)\n",
    "3. [Basic Statistics](#basic-statistics)\n",
    "4. [Text Analysis](#text-analysis)\n",
    "5. [Class Distribution](#class-distribution)\n",
    "6. [Text Preprocessing](#text-preprocessing)\n",
    "7. [Feature Extraction](#feature-extraction)\n",
    "8. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis for text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.preprocessing.text_processor import TextPreprocessor, FeatureExtractor\n",
    "from src.utils.helpers import (\n",
    "    set_seed, plot_word_cloud, plot_text_length_distribution,\n",
    "    plot_class_distribution\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# TODO: Replace with your actual data path\n",
    "# df = pd.read_csv('../data/raw/your_dataset.csv')\n",
    "\n",
    "# For demonstration, create sample data\n",
    "sample_data = {\n",
    "    'text': [\n",
    "        'This product is absolutely amazing! Best purchase ever.',\n",
    "        'Terrible experience. Would not recommend to anyone.',\n",
    "        'Pretty good overall, meets expectations.',\n",
    "        'Worst service I have ever received. Very disappointed.',\n",
    "        'Excellent quality! Highly satisfied with this product.'\n",
    "    ] * 20,\n",
    "    'label': [1, 0, 1, 0, 1] * 20\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nNumber of duplicates: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n",
    "\n",
    "print(\"Text Statistics:\")\n",
    "print(df[['text_length', 'word_count', 'avg_word_length']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot text length distribution\n",
    "plot_text_length_distribution(\n",
    "    df['text'].tolist(),\n",
    "    labels=df['label'].tolist(),\n",
    "    figsize=(12, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud for entire dataset\n",
    "plot_word_cloud(\n",
    "    df['text'].tolist(),\n",
    "    title=\"Word Cloud - All Data\",\n",
    "    max_words=100,\n",
    "    figsize=(14, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = df['label'].value_counts()\n",
    "print(\"Class Distribution:\")\n",
    "print(class_counts)\n",
    "print(f\"\\nClass Balance: {class_counts.min() / class_counts.max():.2f}\")\n",
    "\n",
    "# Plot class distribution\n",
    "plot_class_distribution(\n",
    "    df['label'].values,\n",
    "    class_names=['Negative', 'Positive'],\n",
    "    figsize=(10, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds by class\n",
    "for label in df['label'].unique():\n",
    "    class_texts = df[df['label'] == label]['text'].tolist()\n",
    "    class_name = 'Positive' if label == 1 else 'Negative'\n",
    "    \n",
    "    plot_word_cloud(\n",
    "        class_texts,\n",
    "        title=f\"Word Cloud - {class_name}\",\n",
    "        max_words=50,\n",
    "        figsize=(12, 6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Preprocess sample texts\n",
    "print(\"Original vs Cleaned Text Examples:\\n\")\n",
    "for i in range(3):\n",
    "    original = df['text'].iloc[i]\n",
    "    cleaned = preprocessor.preprocess(original)\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess entire dataset\n",
    "df_processed = preprocessor.preprocess_dataframe(df, 'text', 'label')\n",
    "\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Processed dataset size: {len(df_processed)}\")\n",
    "print(f\"Removed samples: {len(df) - len(df_processed)}\")\n",
    "\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF feature extraction\n",
    "extractor = FeatureExtractor(method='tfidf', max_features=100)\n",
    "X = extractor.fit_transform(df_processed['cleaned_text'])\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(extractor.get_feature_names())}\")\n",
    "\n",
    "# Top features\n",
    "feature_names = extractor.get_feature_names()\n",
    "print(f\"\\nTop 20 features:\")\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "# Calculate average TF-IDF scores\n",
    "mean_tfidf = np.asarray(X.mean(axis=0)).flatten()\n",
    "top_indices = mean_tfidf.argsort()[-20:][::-1]\n",
    "top_features = [feature_names[i] for i in top_indices]\n",
    "top_scores = mean_tfidf[top_indices]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_features)), top_scores, color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel('Average TF-IDF Score', fontsize=12)\n",
    "plt.title('Top 20 Features by TF-IDF Score', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Dataset Size**: [Describe dataset size]\n",
    "2. **Class Balance**: [Describe class distribution]\n",
    "3. **Text Characteristics**: [Describe text length, word count, etc.]\n",
    "4. **Data Quality**: [Describe missing values, duplicates, etc.]\n",
    "5. **Preprocessing Impact**: [Describe preprocessing results]\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Train baseline models (Logistic Regression, Naive Bayes, etc.)\n",
    "2. Implement deep learning models (CNN, LSTM, BERT)\n",
    "3. Hyperparameter tuning\n",
    "4. Model evaluation and comparison\n",
    "5. Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "# df_processed.to_csv('../data/processed/processed_data.csv', index=False)\n",
    "# print(\"Processed data saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
